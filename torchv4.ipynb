{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec40a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7806b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f036d4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n",
      "(569, 30)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "input_size=30\n",
    "hidden_size=500\n",
    "num_classes=2 #iyi huylu tümor veya kötü huylu tümör \n",
    "learning_rate=1e-5 #modelin öğrenme sıklığı 10 üzeri -8  .çok sık ogrenemedi bunu arttırcam\n",
    "num_epoch = 150 #100 adımı vardı 150 .ektim daha fazla öğrensin  ve  yukarıdakini -8i -5b yaptım daha sık iyi öğrenbsin\n",
    "girdi,cikti=load_breast_cancer(return_X_y=True)\n",
    "\n",
    "print(girdi)\n",
    "print(girdi.shape)\n",
    "print(cikti)\n",
    "print(cikti.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5db94dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1-150  Lossf 25.7023\n",
      "Epoch : 2-150  Lossf 25.3471\n",
      "Epoch : 3-150  Lossf 24.9919\n",
      "Epoch : 4-150  Lossf 24.6367\n",
      "Epoch : 5-150  Lossf 24.2815\n",
      "Epoch : 6-150  Lossf 23.9263\n",
      "Epoch : 7-150  Lossf 23.5711\n",
      "Epoch : 8-150  Lossf 23.2159\n",
      "Epoch : 9-150  Lossf 22.8607\n",
      "Epoch : 10-150  Lossf 22.5056\n",
      "Epoch : 11-150  Lossf 22.1504\n",
      "Epoch : 12-150  Lossf 21.7952\n",
      "Epoch : 13-150  Lossf 21.4400\n",
      "Epoch : 14-150  Lossf 21.0848\n",
      "Epoch : 15-150  Lossf 20.7296\n",
      "Epoch : 16-150  Lossf 20.3744\n",
      "Epoch : 17-150  Lossf 20.0192\n",
      "Epoch : 18-150  Lossf 19.6640\n",
      "Epoch : 19-150  Lossf 19.3088\n",
      "Epoch : 20-150  Lossf 18.9537\n",
      "Epoch : 21-150  Lossf 18.5985\n",
      "Epoch : 22-150  Lossf 18.2433\n",
      "Epoch : 23-150  Lossf 17.8881\n",
      "Epoch : 24-150  Lossf 17.5330\n",
      "Epoch : 25-150  Lossf 17.1778\n",
      "Epoch : 26-150  Lossf 16.8227\n",
      "Epoch : 27-150  Lossf 16.4676\n",
      "Epoch : 28-150  Lossf 16.1124\n",
      "Epoch : 29-150  Lossf 15.7573\n",
      "Epoch : 30-150  Lossf 15.4022\n",
      "Epoch : 31-150  Lossf 15.0471\n",
      "Epoch : 32-150  Lossf 14.6920\n",
      "Epoch : 33-150  Lossf 14.3369\n",
      "Epoch : 34-150  Lossf 13.9819\n",
      "Epoch : 35-150  Lossf 13.6269\n",
      "Epoch : 36-150  Lossf 13.2719\n",
      "Epoch : 37-150  Lossf 12.9170\n",
      "Epoch : 38-150  Lossf 12.5621\n",
      "Epoch : 39-150  Lossf 12.2072\n",
      "Epoch : 40-150  Lossf 11.8525\n",
      "Epoch : 41-150  Lossf 11.4978\n",
      "Epoch : 42-150  Lossf 11.1432\n",
      "Epoch : 43-150  Lossf 10.7887\n",
      "Epoch : 44-150  Lossf 10.4344\n",
      "Epoch : 45-150  Lossf 10.0802\n",
      "Epoch : 46-150  Lossf 9.7262\n",
      "Epoch : 47-150  Lossf 9.3725\n",
      "Epoch : 48-150  Lossf 9.0191\n",
      "Epoch : 49-150  Lossf 8.6661\n",
      "Epoch : 50-150  Lossf 8.3135\n",
      "Epoch : 51-150  Lossf 7.9615\n",
      "Epoch : 52-150  Lossf 7.6101\n",
      "Epoch : 53-150  Lossf 7.2596\n",
      "Epoch : 54-150  Lossf 6.9102\n",
      "Epoch : 55-150  Lossf 6.5620\n",
      "Epoch : 56-150  Lossf 6.2156\n",
      "Epoch : 57-150  Lossf 5.8712\n",
      "Epoch : 58-150  Lossf 5.5296\n",
      "Epoch : 59-150  Lossf 5.1917\n",
      "Epoch : 60-150  Lossf 4.8588\n",
      "Epoch : 61-150  Lossf 4.5321\n",
      "Epoch : 62-150  Lossf 4.2131\n",
      "Epoch : 63-150  Lossf 3.9040\n",
      "Epoch : 64-150  Lossf 3.6085\n",
      "Epoch : 65-150  Lossf 3.3298\n",
      "Epoch : 66-150  Lossf 3.0713\n",
      "Epoch : 67-150  Lossf 2.8367\n",
      "Epoch : 68-150  Lossf 2.6285\n",
      "Epoch : 69-150  Lossf 2.4491\n",
      "Epoch : 70-150  Lossf 2.3008\n",
      "Epoch : 71-150  Lossf 2.1838\n",
      "Epoch : 72-150  Lossf 2.0965\n",
      "Epoch : 73-150  Lossf 2.0367\n",
      "Epoch : 74-150  Lossf 2.0016\n",
      "Epoch : 75-150  Lossf 1.9876\n",
      "Epoch : 76-150  Lossf 1.9895\n",
      "Epoch : 77-150  Lossf 2.0025\n",
      "Epoch : 78-150  Lossf 2.0219\n",
      "Epoch : 79-150  Lossf 2.0439\n",
      "Epoch : 80-150  Lossf 2.0654\n",
      "Epoch : 81-150  Lossf 2.0843\n",
      "Epoch : 82-150  Lossf 2.0989\n",
      "Epoch : 83-150  Lossf 2.1083\n",
      "Epoch : 84-150  Lossf 2.1119\n",
      "Epoch : 85-150  Lossf 2.1097\n",
      "Epoch : 86-150  Lossf 2.1020\n",
      "Epoch : 87-150  Lossf 2.0894\n",
      "Epoch : 88-150  Lossf 2.0724\n",
      "Epoch : 89-150  Lossf 2.0521\n",
      "Epoch : 90-150  Lossf 2.0293\n",
      "Epoch : 91-150  Lossf 2.0048\n",
      "Epoch : 92-150  Lossf 1.9796\n",
      "Epoch : 93-150  Lossf 1.9545\n",
      "Epoch : 94-150  Lossf 1.9302\n",
      "Epoch : 95-150  Lossf 1.9073\n",
      "Epoch : 96-150  Lossf 1.8862\n",
      "Epoch : 97-150  Lossf 1.8673\n",
      "Epoch : 98-150  Lossf 1.8507\n",
      "Epoch : 99-150  Lossf 1.8365\n",
      "Epoch : 100-150  Lossf 1.8245\n",
      "Epoch : 101-150  Lossf 1.8146\n",
      "Epoch : 102-150  Lossf 1.8065\n",
      "Epoch : 103-150  Lossf 1.8000\n",
      "Epoch : 104-150  Lossf 1.7945\n",
      "Epoch : 105-150  Lossf 1.7898\n",
      "Epoch : 106-150  Lossf 1.7856\n",
      "Epoch : 107-150  Lossf 1.7815\n",
      "Epoch : 108-150  Lossf 1.7774\n",
      "Epoch : 109-150  Lossf 1.7730\n",
      "Epoch : 110-150  Lossf 1.7682\n",
      "Epoch : 111-150  Lossf 1.7629\n",
      "Epoch : 112-150  Lossf 1.7571\n",
      "Epoch : 113-150  Lossf 1.7509\n",
      "Epoch : 114-150  Lossf 1.7442\n",
      "Epoch : 115-150  Lossf 1.7372\n",
      "Epoch : 116-150  Lossf 1.7300\n",
      "Epoch : 117-150  Lossf 1.7226\n",
      "Epoch : 118-150  Lossf 1.7152\n",
      "Epoch : 119-150  Lossf 1.7078\n",
      "Epoch : 120-150  Lossf 1.7005\n",
      "Epoch : 121-150  Lossf 1.6934\n",
      "Epoch : 122-150  Lossf 1.6865\n",
      "Epoch : 123-150  Lossf 1.6799\n",
      "Epoch : 124-150  Lossf 1.6734\n",
      "Epoch : 125-150  Lossf 1.6672\n",
      "Epoch : 126-150  Lossf 1.6612\n",
      "Epoch : 127-150  Lossf 1.6553\n",
      "Epoch : 128-150  Lossf 1.6496\n",
      "Epoch : 129-150  Lossf 1.6440\n",
      "Epoch : 130-150  Lossf 1.6385\n",
      "Epoch : 131-150  Lossf 1.6330\n",
      "Epoch : 132-150  Lossf 1.6275\n",
      "Epoch : 133-150  Lossf 1.6221\n",
      "Epoch : 134-150  Lossf 1.6166\n",
      "Epoch : 135-150  Lossf 1.6111\n",
      "Epoch : 136-150  Lossf 1.6056\n",
      "Epoch : 137-150  Lossf 1.6001\n",
      "Epoch : 138-150  Lossf 1.5945\n",
      "Epoch : 139-150  Lossf 1.5890\n",
      "Epoch : 140-150  Lossf 1.5834\n",
      "Epoch : 141-150  Lossf 1.5779\n",
      "Epoch : 142-150  Lossf 1.5724\n",
      "Epoch : 143-150  Lossf 1.5669\n",
      "Epoch : 144-150  Lossf 1.5614\n",
      "Epoch : 145-150  Lossf 1.5560\n",
      "Epoch : 146-150  Lossf 1.5506\n",
      "Epoch : 147-150  Lossf 1.5452\n",
      "Epoch : 148-150  Lossf 1.5400\n",
      "Epoch : 149-150  Lossf 1.5347\n",
      "Epoch : 150-150  Lossf 1.5295\n"
     ]
    }
   ],
   "source": [
    "train_input=torch.from_numpy(girdi).float()\n",
    "train_output=torch.from_numpy(cikti).long()\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.fc1=nn.Linear(input_size,hidden_size)\n",
    "        self.lrelu=nn.LeakyReLU(negative_slope=0.02)\n",
    "        self.fc2=nn.Linear(hidden_size,num_classes)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        outfc1=self.fc1(input)\n",
    "        outfc1relu=self.lrelu(outfc1)\n",
    "        out=self.fc2(outfc1relu)\n",
    "        return out\n",
    "    \n",
    "model=NeuralNetwork(input_size,hidden_size,num_classes)\n",
    "\n",
    "\n",
    "\n",
    "lossf=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "        outputs=model(train_input)\n",
    "        loss=lossf(outputs,train_output)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step() #optimizasyon tanıumlanır    her seferinde opt. sıfırla ağırlık gradyan güncelle  sonra opt. tanımla\n",
    "        #eğer sıfırlanamzsam türevleri sürekli üzerine ekleyerek gider\n",
    "        \n",
    "    \n",
    "        print(\"Epoch : {}-{}  Lossf {:.4f}\".format(epoch+1,num_epoch,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sınıf sayısı 1 ve dah fazlaysa cross ent. kullanabiliriz  toplamları 1 olacak şekilde her sınıfı yüzde üzerinden o girdinin\n",
    "#kaç olabileceğini verir\n",
    "#modelinizin tahmin ettiği olasılıklar ile gerçek sınıf etiketleri arasındaki uyumsuzluğu ölçer.minimize etmek.\n",
    "\n",
    "#softmmax Softmax, bir dizi sayıyı (genellikle logits adı verilen ham skorları), her bir elemanın toplamda 1 olacak şekilde \n",
    "#bir olasılık  dağılımına dönüştüren matematiksel bir fonksiyondur. Özellikle sınıflandırma problemlerinde, modelin her bir sınıf \n",
    "#için tahmin ettiği olasılıkları hesaplamak için kullanılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÖNEMLİ Aktivasyon fonksiyonlarıdır \n",
    "Softmax, Sigmoid ve diğer aktivasyon fonksiyonları, derin öğrenme modellerinde tahmin yapmayı kolaylaştıran matematiksel\n",
    "araçlardır. Her bir fonksiyonun temel amacı, modelin çıktısını işleyerek belirli bir forma (örneğin olasılık dağılımı) \n",
    "dönüştürmek ve daha iyi bir öğrenme süreci sağlamaktır. Şimdi bu fonksiyonları ve aralarındaki farkları detaylıca inceleyelim.\n",
    "\n",
    "1. Aktivasyon Fonksiyonlarının Genel Rolü\n",
    "Aktivasyon fonksiyonları, yapay sinir ağlarında her bir nöronun çıktısını belirlemek için kullanılır. Özellikle:\n",
    "\n",
    "Doğrusal olmayanlık ekleyerek modeli daha güçlü hale getirir.\n",
    "Modelin belirli bir problem (sınıflandırma, regresyon vb.) için çıktısını uygun bir şekilde dönüştürür.\n",
    "Tahmin sonucunu normalize ederek anlamlı hale getirir (örneğin, olasılık)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f10252",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Sigmoid Fonksiyonu\n",
    "Sigmoid, bir sayıyı \n",
    "0\n",
    "0 ile \n",
    "1\n",
    "1 arasında bir değere dönüştüren fonksiyondur.\n",
    "\n",
    "Kullanım Alanı:\n",
    "İkili sınıflandırma problemlerinde (örneğin, hastalık var mı yok mu).\n",
    "Çok etiketli sınıflandırma (multi-label classification), çünkü her sınıf birbirinden bağımsız değerlendirilir.\n",
    "Örnek:\n",
    "Bir modelin çıktısı: \n",
    "2.0\n",
    "2.0.\n",
    "Sigmoid sonrası: \n",
    "0.88\n",
    "0.88.\n",
    "Bu, girdinin %88 ihtimalle pozitif sınıfa ait olduğunu gösterir.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c56e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLU, doğrusal olmayan bir fonksiyondur ve negatif değerleri sıfıra çeker, pozitif değerleri olduğu gibi bırakır.\n",
    "Kullanım Alanı:\n",
    "Gizli katmanlarda, özellikle derin öğrenme modellerinde öğrenmeyi hızlandırmak için.\n",
    "Negatif çıktıları sıfırlayarak sparsity (seyreklik) sağlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a95ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Softmax, genellikle sınıflandırma problemlerinde son katman olarak kullanılır. Örneğin, bir model bir görüntünün 3 farklı \n",
    "sınıfa (kedi, köpek, kuş) ait olma olasılıklarını tahmin edebilir.\n",
    "\n",
    "Örnek:\n",
    "\n",
    "python\n",
    "Kodu kopyala\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Logits (modelin çıktısı)\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "\n",
    "# Softmax fonksiyonu\n",
    "probabilities = F.softmax(logits, dim=0)\n",
    "\n",
    "print(probabilities)  # Sınıf olasılıkları toplamı 1 olacak\n",
    "Çıktı örneği:\n",
    "\n",
    "scss\n",
    "Kodu kopyala\n",
    "tensor([0.6590, 0.2424, 0.0986])\n",
    "Bu, girişin %65.9 ihtimalle birinci sınıfa, %24.2 ihtimalle ikinci sınıfa, %9.86 ihtimalle üçüncü sınıfa ait olduğunu gösterir.\n",
    "\n",
    "Softmax vs. Sigmoid:\n",
    "Softmax: Birden fazla sınıf arasında seçim yapmak için kullanılır. Sınıflar birbirini dışlar (mutually exclusive).\n",
    "Sigmoid: Her sınıfın birbirinden bağımsız olduğu durumlarda (örneğin, çok etiketli sınıflandırma) kullanılır.\n",
    "Avantajları:\n",
    "Birden fazla sınıfa ait olasılık tahminleri yapmayı sağlar.\n",
    "Modelin çıktısını daha kolay yorumlamaya olanak tanır.\n",
    "Özet:\n",
    "Softmax, sınıflandırma problemlerinde modelin her bir sınıf için tahmin ettiği olasılıkları hesaplamakta kullanılan bir fonksiyondur\n",
    ". Modelin tahminlerinin toplamının 1 olmasını sağlar, bu da olasılık dağılımı üretmeyi kolaylaştırır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5be66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aktivasyon fonksiyonları doğrudan tahmin yapmaz, ancak modelin tahminlerini anlamlı hale getirir:  yani biz anlayalım diye\n",
    "    saçma sağan değer yazdırmak yerine %60 doğruluk oranı olduğunu yazdıırıyoor\n",
    "\n",
    "Softmax, sınıfların olasılıklarını üretir. Tahmin edilen sınıf, en yüksek olasılığı olan sınıftır.\n",
    "Sigmoid, pozitif sınıfa ait olma olasılığını verir.\n",
    "ReLU ve Tanh, daha çok ara katmanlarda kullanılır; tahmin sonuçlarıyla doğrudan bağlantılı değildir.\n",
    "Özet\n",
    "Softmax: Çok sınıflı sınıflandırmada, sınıf olasılıklarını oluşturur.\n",
    "Sigmoid: İkili sınıflandırmada veya bağımsız çok etiketli problemler için kullanılır.\n",
    "ReLU ve Tanh: Daha çok öğrenme sürecini hızlandırmak ve doğrusal olmayanlık eklemek için gizli katmanlarda kullanılır.\n",
    "Bu fonksiyonlar, modelin çıktısını işleyerek öğrenmeyi ve tahmini daha etkili hale getirir. Doğru fonksiyon seçimi, problem türüne bağlıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae39b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2aa2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizerlardır bi nevi bu backprobagationda olan kayıp fonk. da gelen hata oranını minilize etmek ağırlıkları güncelleme \n",
    "#işlemlerinin arkaplanda asıl yapanlardan biridir \n",
    "\n",
    "Evet, optimizer, backpropagation sırasında hesaplanan gradyanları (gradient) kullanarak model parametrelerini güncellemekten\n",
    "sorumludur. Ancak tek başına tüm işlemleri yapan bir bileşen değildir.\n",
    "İşlem zincirinin bir parçasıdır. Bu zinciri adım adım açıklayalım:\n",
    "\n",
    "Forward Pass\n",
    "Ne olur?\n",
    "\n",
    "Model, giriş verileri üzerinden bir tahmin (prediction) yapar.   ilk verilerden tahmin yaptı öylesine (ama uygun tahmin)ve ileriye dönük \n",
    "Bu tahmin, ileri yönlü bir hesaplama (forward pass) sonucunda oluşur.\n",
    "Aktif Eleman: Modelin katmanları ve aktivasyon fonksiyonlarıdır.\n",
    "Örneğin, model(inputs) çağrıldığında, bu işlem gerçekleşir.\n",
    "\n",
    "Loss Function\n",
    "Ne olur?\n",
    "Modelin tahminleri (preds) ve gerçek hedefler (targets) arasındaki fark hesaplanır.\n",
    "Bu fark, bir kayıp fonksiyonu (loss function) tarafından bir ölçüye dönüştürülür.\n",
    "Örneğin, MSE, CrossEntropyLoss, BinaryCrossEntropy.\n",
    "Aktif Eleman: Loss function.\n",
    "    \n",
    "Backward Pass (Backpropagation)\n",
    "Ne olur?\n",
    "Kayıp fonksiyonu, model parametrelerine (ağırlıklar ve biaslar) göre türevlenir (gradyanlar hesaplanır).\n",
    "Bu işlem, otomatik türevleme (autograd) mekanizması tarafından gerçekleştirilir.\n",
    "Aktif Elemanlar: PyTorch gibi framework'lerin loss.backward() metodu.\n",
    "Bu aşamada, gradyanlar w.grad ve b.grad gibi değişkenlerde saklanır.\n",
    "\n",
    "Optimizer\n",
    "Ne yapar?\n",
    "Backpropagation'ın ürettiği gradyanları kullanarak model parametrelerini günceller.\n",
    "Gradyanlar, optimizer'ın algoritmasına (SGD, Adam, RMSprop vb.) bağlı olarak işlenir.\n",
    "Formül:\n",
    "𝜃\n",
    "=\n",
    "𝜃\n",
    "−\n",
    "𝜂\n",
    "⋅\n",
    "∇\n",
    "𝐽\n",
    "(\n",
    "𝜃\n",
    ")\n",
    "θ=θ−η⋅∇J(θ)\n",
    "𝜃\n",
    "θ: Model parametreleri (örneğin ağırlıklar, biaslar).\n",
    "𝜂\n",
    "η: Öğrenme oranı.\n",
    "∇\n",
    "𝐽\n",
    "(\n",
    "𝜃\n",
    ")\n",
    "∇J(θ): Kayıp fonksiyonunun gradyanı.\n",
    "Aktif Elemanlar: opt.step().\n",
    "Optimizer'ın Rolü\n",
    "Backpropagation Sonrası Gelen İşlem:\n",
    "Backpropagation, sadece gradyanları hesaplar. Modelin parametrelerini değiştirmez.\n",
    "Optimizasyon işlemi, bu gradyanları kullanarak parametreleri günceller.\n",
    "Kaybı Minimize Etmek:\n",
    "Gradyanlar, kayıp fonksiyonunun eğimini temsil eder. Optimizer, bu eğimi takip ederek kaybı minimize eder.\n",
    "Model bu şekilde daha doğru tahminler yapmayı öğrenir.\n",
    "Forward, Loss, Backward ve Optimizer İlişkisi\n",
    "Bu işlemleri bir araya getirirsek:\n",
    "\n",
    "Forward Pass:\n",
    "Model tahmin yapar. preds = model(inputs)\n",
    "Loss Hesaplama:\n",
    "Kayıp hesaplanır. loss = loss_fn(preds, targets)\n",
    "Backward Pass:\n",
    "Gradyanlar hesaplanır. loss.backward()\n",
    "Optimizer:\n",
    "Parametreler güncellenir. opt.step()\n",
    "Gradyanlar sıfırlanır. opt.zero_grad()\n",
    "Sonuç\n",
    "Optimizer, backpropagation'dan sonra gradyanları kullanarak parametreleri güncelleyen bileşendir.\n",
    "Ancak tahmin işlemini yapan forward pass, kaybı hesaplayan loss function, ve gradyanları hesaplayan backpropagation birbirin\n",
    "den bağımsız çalışır ama birbiriyle ilişkilidir. Optimizasyon işlemi, tüm bu adımları bir araya getirerek modeli eğitir.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
