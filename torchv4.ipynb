{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec40a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7806b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f036d4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n",
      "(569, 30)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "input_size=30\n",
    "hidden_size=500\n",
    "num_classes=2 #iyi huylu tÃ¼mor veya kÃ¶tÃ¼ huylu tÃ¼mÃ¶r \n",
    "learning_rate=1e-5 #modelin Ã¶ÄŸrenme sÄ±klÄ±ÄŸÄ± 10 Ã¼zeri -8  .Ã§ok sÄ±k ogrenemedi bunu arttÄ±rcam\n",
    "num_epoch = 150 #100 adÄ±mÄ± vardÄ± 150 .ektim daha fazla Ã¶ÄŸrensin  ve  yukarÄ±dakini -8i -5b yaptÄ±m daha sÄ±k iyi Ã¶ÄŸrenbsin\n",
    "girdi,cikti=load_breast_cancer(return_X_y=True)\n",
    "\n",
    "print(girdi)\n",
    "print(girdi.shape)\n",
    "print(cikti)\n",
    "print(cikti.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5db94dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1-150  Lossf 25.7023\n",
      "Epoch : 2-150  Lossf 25.3471\n",
      "Epoch : 3-150  Lossf 24.9919\n",
      "Epoch : 4-150  Lossf 24.6367\n",
      "Epoch : 5-150  Lossf 24.2815\n",
      "Epoch : 6-150  Lossf 23.9263\n",
      "Epoch : 7-150  Lossf 23.5711\n",
      "Epoch : 8-150  Lossf 23.2159\n",
      "Epoch : 9-150  Lossf 22.8607\n",
      "Epoch : 10-150  Lossf 22.5056\n",
      "Epoch : 11-150  Lossf 22.1504\n",
      "Epoch : 12-150  Lossf 21.7952\n",
      "Epoch : 13-150  Lossf 21.4400\n",
      "Epoch : 14-150  Lossf 21.0848\n",
      "Epoch : 15-150  Lossf 20.7296\n",
      "Epoch : 16-150  Lossf 20.3744\n",
      "Epoch : 17-150  Lossf 20.0192\n",
      "Epoch : 18-150  Lossf 19.6640\n",
      "Epoch : 19-150  Lossf 19.3088\n",
      "Epoch : 20-150  Lossf 18.9537\n",
      "Epoch : 21-150  Lossf 18.5985\n",
      "Epoch : 22-150  Lossf 18.2433\n",
      "Epoch : 23-150  Lossf 17.8881\n",
      "Epoch : 24-150  Lossf 17.5330\n",
      "Epoch : 25-150  Lossf 17.1778\n",
      "Epoch : 26-150  Lossf 16.8227\n",
      "Epoch : 27-150  Lossf 16.4676\n",
      "Epoch : 28-150  Lossf 16.1124\n",
      "Epoch : 29-150  Lossf 15.7573\n",
      "Epoch : 30-150  Lossf 15.4022\n",
      "Epoch : 31-150  Lossf 15.0471\n",
      "Epoch : 32-150  Lossf 14.6920\n",
      "Epoch : 33-150  Lossf 14.3369\n",
      "Epoch : 34-150  Lossf 13.9819\n",
      "Epoch : 35-150  Lossf 13.6269\n",
      "Epoch : 36-150  Lossf 13.2719\n",
      "Epoch : 37-150  Lossf 12.9170\n",
      "Epoch : 38-150  Lossf 12.5621\n",
      "Epoch : 39-150  Lossf 12.2072\n",
      "Epoch : 40-150  Lossf 11.8525\n",
      "Epoch : 41-150  Lossf 11.4978\n",
      "Epoch : 42-150  Lossf 11.1432\n",
      "Epoch : 43-150  Lossf 10.7887\n",
      "Epoch : 44-150  Lossf 10.4344\n",
      "Epoch : 45-150  Lossf 10.0802\n",
      "Epoch : 46-150  Lossf 9.7262\n",
      "Epoch : 47-150  Lossf 9.3725\n",
      "Epoch : 48-150  Lossf 9.0191\n",
      "Epoch : 49-150  Lossf 8.6661\n",
      "Epoch : 50-150  Lossf 8.3135\n",
      "Epoch : 51-150  Lossf 7.9615\n",
      "Epoch : 52-150  Lossf 7.6101\n",
      "Epoch : 53-150  Lossf 7.2596\n",
      "Epoch : 54-150  Lossf 6.9102\n",
      "Epoch : 55-150  Lossf 6.5620\n",
      "Epoch : 56-150  Lossf 6.2156\n",
      "Epoch : 57-150  Lossf 5.8712\n",
      "Epoch : 58-150  Lossf 5.5296\n",
      "Epoch : 59-150  Lossf 5.1917\n",
      "Epoch : 60-150  Lossf 4.8588\n",
      "Epoch : 61-150  Lossf 4.5321\n",
      "Epoch : 62-150  Lossf 4.2131\n",
      "Epoch : 63-150  Lossf 3.9040\n",
      "Epoch : 64-150  Lossf 3.6085\n",
      "Epoch : 65-150  Lossf 3.3298\n",
      "Epoch : 66-150  Lossf 3.0713\n",
      "Epoch : 67-150  Lossf 2.8367\n",
      "Epoch : 68-150  Lossf 2.6285\n",
      "Epoch : 69-150  Lossf 2.4491\n",
      "Epoch : 70-150  Lossf 2.3008\n",
      "Epoch : 71-150  Lossf 2.1838\n",
      "Epoch : 72-150  Lossf 2.0965\n",
      "Epoch : 73-150  Lossf 2.0367\n",
      "Epoch : 74-150  Lossf 2.0016\n",
      "Epoch : 75-150  Lossf 1.9876\n",
      "Epoch : 76-150  Lossf 1.9895\n",
      "Epoch : 77-150  Lossf 2.0025\n",
      "Epoch : 78-150  Lossf 2.0219\n",
      "Epoch : 79-150  Lossf 2.0439\n",
      "Epoch : 80-150  Lossf 2.0654\n",
      "Epoch : 81-150  Lossf 2.0843\n",
      "Epoch : 82-150  Lossf 2.0989\n",
      "Epoch : 83-150  Lossf 2.1083\n",
      "Epoch : 84-150  Lossf 2.1119\n",
      "Epoch : 85-150  Lossf 2.1097\n",
      "Epoch : 86-150  Lossf 2.1020\n",
      "Epoch : 87-150  Lossf 2.0894\n",
      "Epoch : 88-150  Lossf 2.0724\n",
      "Epoch : 89-150  Lossf 2.0521\n",
      "Epoch : 90-150  Lossf 2.0293\n",
      "Epoch : 91-150  Lossf 2.0048\n",
      "Epoch : 92-150  Lossf 1.9796\n",
      "Epoch : 93-150  Lossf 1.9545\n",
      "Epoch : 94-150  Lossf 1.9302\n",
      "Epoch : 95-150  Lossf 1.9073\n",
      "Epoch : 96-150  Lossf 1.8862\n",
      "Epoch : 97-150  Lossf 1.8673\n",
      "Epoch : 98-150  Lossf 1.8507\n",
      "Epoch : 99-150  Lossf 1.8365\n",
      "Epoch : 100-150  Lossf 1.8245\n",
      "Epoch : 101-150  Lossf 1.8146\n",
      "Epoch : 102-150  Lossf 1.8065\n",
      "Epoch : 103-150  Lossf 1.8000\n",
      "Epoch : 104-150  Lossf 1.7945\n",
      "Epoch : 105-150  Lossf 1.7898\n",
      "Epoch : 106-150  Lossf 1.7856\n",
      "Epoch : 107-150  Lossf 1.7815\n",
      "Epoch : 108-150  Lossf 1.7774\n",
      "Epoch : 109-150  Lossf 1.7730\n",
      "Epoch : 110-150  Lossf 1.7682\n",
      "Epoch : 111-150  Lossf 1.7629\n",
      "Epoch : 112-150  Lossf 1.7571\n",
      "Epoch : 113-150  Lossf 1.7509\n",
      "Epoch : 114-150  Lossf 1.7442\n",
      "Epoch : 115-150  Lossf 1.7372\n",
      "Epoch : 116-150  Lossf 1.7300\n",
      "Epoch : 117-150  Lossf 1.7226\n",
      "Epoch : 118-150  Lossf 1.7152\n",
      "Epoch : 119-150  Lossf 1.7078\n",
      "Epoch : 120-150  Lossf 1.7005\n",
      "Epoch : 121-150  Lossf 1.6934\n",
      "Epoch : 122-150  Lossf 1.6865\n",
      "Epoch : 123-150  Lossf 1.6799\n",
      "Epoch : 124-150  Lossf 1.6734\n",
      "Epoch : 125-150  Lossf 1.6672\n",
      "Epoch : 126-150  Lossf 1.6612\n",
      "Epoch : 127-150  Lossf 1.6553\n",
      "Epoch : 128-150  Lossf 1.6496\n",
      "Epoch : 129-150  Lossf 1.6440\n",
      "Epoch : 130-150  Lossf 1.6385\n",
      "Epoch : 131-150  Lossf 1.6330\n",
      "Epoch : 132-150  Lossf 1.6275\n",
      "Epoch : 133-150  Lossf 1.6221\n",
      "Epoch : 134-150  Lossf 1.6166\n",
      "Epoch : 135-150  Lossf 1.6111\n",
      "Epoch : 136-150  Lossf 1.6056\n",
      "Epoch : 137-150  Lossf 1.6001\n",
      "Epoch : 138-150  Lossf 1.5945\n",
      "Epoch : 139-150  Lossf 1.5890\n",
      "Epoch : 140-150  Lossf 1.5834\n",
      "Epoch : 141-150  Lossf 1.5779\n",
      "Epoch : 142-150  Lossf 1.5724\n",
      "Epoch : 143-150  Lossf 1.5669\n",
      "Epoch : 144-150  Lossf 1.5614\n",
      "Epoch : 145-150  Lossf 1.5560\n",
      "Epoch : 146-150  Lossf 1.5506\n",
      "Epoch : 147-150  Lossf 1.5452\n",
      "Epoch : 148-150  Lossf 1.5400\n",
      "Epoch : 149-150  Lossf 1.5347\n",
      "Epoch : 150-150  Lossf 1.5295\n"
     ]
    }
   ],
   "source": [
    "train_input=torch.from_numpy(girdi).float()\n",
    "train_output=torch.from_numpy(cikti).long()\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.fc1=nn.Linear(input_size,hidden_size)\n",
    "        self.lrelu=nn.LeakyReLU(negative_slope=0.02)\n",
    "        self.fc2=nn.Linear(hidden_size,num_classes)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        outfc1=self.fc1(input)\n",
    "        outfc1relu=self.lrelu(outfc1)\n",
    "        out=self.fc2(outfc1relu)\n",
    "        return out\n",
    "    \n",
    "model=NeuralNetwork(input_size,hidden_size,num_classes)\n",
    "\n",
    "\n",
    "\n",
    "lossf=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "        outputs=model(train_input)\n",
    "        loss=lossf(outputs,train_output)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step() #optimizasyon tanÄ±umlanÄ±r    her seferinde opt. sÄ±fÄ±rla aÄŸÄ±rlÄ±k gradyan gÃ¼ncelle  sonra opt. tanÄ±mla\n",
    "        #eÄŸer sÄ±fÄ±rlanamzsam tÃ¼revleri sÃ¼rekli Ã¼zerine ekleyerek gider\n",
    "        \n",
    "    \n",
    "        print(\"Epoch : {}-{}  Lossf {:.4f}\".format(epoch+1,num_epoch,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sÄ±nÄ±f sayÄ±sÄ± 1 ve dah fazlaysa cross ent. kullanabiliriz  toplamlarÄ± 1 olacak ÅŸekilde her sÄ±nÄ±fÄ± yÃ¼zde Ã¼zerinden o girdinin\n",
    "#kaÃ§ olabileceÄŸini verir\n",
    "#modelinizin tahmin ettiÄŸi olasÄ±lÄ±klar ile gerÃ§ek sÄ±nÄ±f etiketleri arasÄ±ndaki uyumsuzluÄŸu Ã¶lÃ§er.minimize etmek.\n",
    "\n",
    "#softmmax Softmax, bir dizi sayÄ±yÄ± (genellikle logits adÄ± verilen ham skorlarÄ±), her bir elemanÄ±n toplamda 1 olacak ÅŸekilde \n",
    "#bir olasÄ±lÄ±k  daÄŸÄ±lÄ±mÄ±na dÃ¶nÃ¼ÅŸtÃ¼ren matematiksel bir fonksiyondur. Ã–zellikle sÄ±nÄ±flandÄ±rma problemlerinde, modelin her bir sÄ±nÄ±f \n",
    "#iÃ§in tahmin ettiÄŸi olasÄ±lÄ±klarÄ± hesaplamak iÃ§in kullanÄ±lÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ã–NEMLÄ° Aktivasyon fonksiyonlarÄ±dÄ±r \n",
    "Softmax, Sigmoid ve diÄŸer aktivasyon fonksiyonlarÄ±, derin Ã¶ÄŸrenme modellerinde tahmin yapmayÄ± kolaylaÅŸtÄ±ran matematiksel\n",
    "araÃ§lardÄ±r. Her bir fonksiyonun temel amacÄ±, modelin Ã§Ä±ktÄ±sÄ±nÄ± iÅŸleyerek belirli bir forma (Ã¶rneÄŸin olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±) \n",
    "dÃ¶nÃ¼ÅŸtÃ¼rmek ve daha iyi bir Ã¶ÄŸrenme sÃ¼reci saÄŸlamaktÄ±r. Åimdi bu fonksiyonlarÄ± ve aralarÄ±ndaki farklarÄ± detaylÄ±ca inceleyelim.\n",
    "\n",
    "1. Aktivasyon FonksiyonlarÄ±nÄ±n Genel RolÃ¼\n",
    "Aktivasyon fonksiyonlarÄ±, yapay sinir aÄŸlarÄ±nda her bir nÃ¶ronun Ã§Ä±ktÄ±sÄ±nÄ± belirlemek iÃ§in kullanÄ±lÄ±r. Ã–zellikle:\n",
    "\n",
    "DoÄŸrusal olmayanlÄ±k ekleyerek modeli daha gÃ¼Ã§lÃ¼ hale getirir.\n",
    "Modelin belirli bir problem (sÄ±nÄ±flandÄ±rma, regresyon vb.) iÃ§in Ã§Ä±ktÄ±sÄ±nÄ± uygun bir ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\n",
    "Tahmin sonucunu normalize ederek anlamlÄ± hale getirir (Ã¶rneÄŸin, olasÄ±lÄ±k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f10252",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Sigmoid Fonksiyonu\n",
    "Sigmoid, bir sayÄ±yÄ± \n",
    "0\n",
    "0 ile \n",
    "1\n",
    "1 arasÄ±nda bir deÄŸere dÃ¶nÃ¼ÅŸtÃ¼ren fonksiyondur.\n",
    "\n",
    "KullanÄ±m AlanÄ±:\n",
    "Ä°kili sÄ±nÄ±flandÄ±rma problemlerinde (Ã¶rneÄŸin, hastalÄ±k var mÄ± yok mu).\n",
    "Ã‡ok etiketli sÄ±nÄ±flandÄ±rma (multi-label classification), Ã§Ã¼nkÃ¼ her sÄ±nÄ±f birbirinden baÄŸÄ±msÄ±z deÄŸerlendirilir.\n",
    "Ã–rnek:\n",
    "Bir modelin Ã§Ä±ktÄ±sÄ±: \n",
    "2.0\n",
    "2.0.\n",
    "Sigmoid sonrasÄ±: \n",
    "0.88\n",
    "0.88.\n",
    "Bu, girdinin %88 ihtimalle pozitif sÄ±nÄ±fa ait olduÄŸunu gÃ¶sterir.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c56e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLU, doÄŸrusal olmayan bir fonksiyondur ve negatif deÄŸerleri sÄ±fÄ±ra Ã§eker, pozitif deÄŸerleri olduÄŸu gibi bÄ±rakÄ±r.\n",
    "KullanÄ±m AlanÄ±:\n",
    "Gizli katmanlarda, Ã¶zellikle derin Ã¶ÄŸrenme modellerinde Ã¶ÄŸrenmeyi hÄ±zlandÄ±rmak iÃ§in.\n",
    "Negatif Ã§Ä±ktÄ±larÄ± sÄ±fÄ±rlayarak sparsity (seyreklik) saÄŸlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a95ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Softmax, genellikle sÄ±nÄ±flandÄ±rma problemlerinde son katman olarak kullanÄ±lÄ±r. Ã–rneÄŸin, bir model bir gÃ¶rÃ¼ntÃ¼nÃ¼n 3 farklÄ± \n",
    "sÄ±nÄ±fa (kedi, kÃ¶pek, kuÅŸ) ait olma olasÄ±lÄ±klarÄ±nÄ± tahmin edebilir.\n",
    "\n",
    "Ã–rnek:\n",
    "\n",
    "python\n",
    "Kodu kopyala\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Logits (modelin Ã§Ä±ktÄ±sÄ±)\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "\n",
    "# Softmax fonksiyonu\n",
    "probabilities = F.softmax(logits, dim=0)\n",
    "\n",
    "print(probabilities)  # SÄ±nÄ±f olasÄ±lÄ±klarÄ± toplamÄ± 1 olacak\n",
    "Ã‡Ä±ktÄ± Ã¶rneÄŸi:\n",
    "\n",
    "scss\n",
    "Kodu kopyala\n",
    "tensor([0.6590, 0.2424, 0.0986])\n",
    "Bu, giriÅŸin %65.9 ihtimalle birinci sÄ±nÄ±fa, %24.2 ihtimalle ikinci sÄ±nÄ±fa, %9.86 ihtimalle Ã¼Ã§Ã¼ncÃ¼ sÄ±nÄ±fa ait olduÄŸunu gÃ¶sterir.\n",
    "\n",
    "Softmax vs. Sigmoid:\n",
    "Softmax: Birden fazla sÄ±nÄ±f arasÄ±nda seÃ§im yapmak iÃ§in kullanÄ±lÄ±r. SÄ±nÄ±flar birbirini dÄ±ÅŸlar (mutually exclusive).\n",
    "Sigmoid: Her sÄ±nÄ±fÄ±n birbirinden baÄŸÄ±msÄ±z olduÄŸu durumlarda (Ã¶rneÄŸin, Ã§ok etiketli sÄ±nÄ±flandÄ±rma) kullanÄ±lÄ±r.\n",
    "AvantajlarÄ±:\n",
    "Birden fazla sÄ±nÄ±fa ait olasÄ±lÄ±k tahminleri yapmayÄ± saÄŸlar.\n",
    "Modelin Ã§Ä±ktÄ±sÄ±nÄ± daha kolay yorumlamaya olanak tanÄ±r.\n",
    "Ã–zet:\n",
    "Softmax, sÄ±nÄ±flandÄ±rma problemlerinde modelin her bir sÄ±nÄ±f iÃ§in tahmin ettiÄŸi olasÄ±lÄ±klarÄ± hesaplamakta kullanÄ±lan bir fonksiyondur\n",
    ". Modelin tahminlerinin toplamÄ±nÄ±n 1 olmasÄ±nÄ± saÄŸlar, bu da olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± Ã¼retmeyi kolaylaÅŸtÄ±rÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5be66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aktivasyon fonksiyonlarÄ± doÄŸrudan tahmin yapmaz, ancak modelin tahminlerini anlamlÄ± hale getirir:  yani biz anlayalÄ±m diye\n",
    "    saÃ§ma saÄŸan deÄŸer yazdÄ±rmak yerine %60 doÄŸruluk oranÄ± olduÄŸunu yazdÄ±Ä±rÄ±yoor\n",
    "\n",
    "Softmax, sÄ±nÄ±flarÄ±n olasÄ±lÄ±klarÄ±nÄ± Ã¼retir. Tahmin edilen sÄ±nÄ±f, en yÃ¼ksek olasÄ±lÄ±ÄŸÄ± olan sÄ±nÄ±ftÄ±r.\n",
    "Sigmoid, pozitif sÄ±nÄ±fa ait olma olasÄ±lÄ±ÄŸÄ±nÄ± verir.\n",
    "ReLU ve Tanh, daha Ã§ok ara katmanlarda kullanÄ±lÄ±r; tahmin sonuÃ§larÄ±yla doÄŸrudan baÄŸlantÄ±lÄ± deÄŸildir.\n",
    "Ã–zet\n",
    "Softmax: Ã‡ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rmada, sÄ±nÄ±f olasÄ±lÄ±klarÄ±nÄ± oluÅŸturur.\n",
    "Sigmoid: Ä°kili sÄ±nÄ±flandÄ±rmada veya baÄŸÄ±msÄ±z Ã§ok etiketli problemler iÃ§in kullanÄ±lÄ±r.\n",
    "ReLU ve Tanh: Daha Ã§ok Ã¶ÄŸrenme sÃ¼recini hÄ±zlandÄ±rmak ve doÄŸrusal olmayanlÄ±k eklemek iÃ§in gizli katmanlarda kullanÄ±lÄ±r.\n",
    "Bu fonksiyonlar, modelin Ã§Ä±ktÄ±sÄ±nÄ± iÅŸleyerek Ã¶ÄŸrenmeyi ve tahmini daha etkili hale getirir. DoÄŸru fonksiyon seÃ§imi, problem tÃ¼rÃ¼ne baÄŸlÄ±dÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae39b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2aa2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizerlardÄ±r bi nevi bu backprobagationda olan kayÄ±p fonk. da gelen hata oranÄ±nÄ± minilize etmek aÄŸÄ±rlÄ±klarÄ± gÃ¼ncelleme \n",
    "#iÅŸlemlerinin arkaplanda asÄ±l yapanlardan biridir \n",
    "\n",
    "Evet, optimizer, backpropagation sÄ±rasÄ±nda hesaplanan gradyanlarÄ± (gradient) kullanarak model parametrelerini gÃ¼ncellemekten\n",
    "sorumludur. Ancak tek baÅŸÄ±na tÃ¼m iÅŸlemleri yapan bir bileÅŸen deÄŸildir.\n",
    "Ä°ÅŸlem zincirinin bir parÃ§asÄ±dÄ±r. Bu zinciri adÄ±m adÄ±m aÃ§Ä±klayalÄ±m:\n",
    "\n",
    "Forward Pass\n",
    "Ne olur?\n",
    "\n",
    "Model, giriÅŸ verileri Ã¼zerinden bir tahmin (prediction) yapar.   ilk verilerden tahmin yaptÄ± Ã¶ylesine (ama uygun tahmin)ve ileriye dÃ¶nÃ¼k \n",
    "Bu tahmin, ileri yÃ¶nlÃ¼ bir hesaplama (forward pass) sonucunda oluÅŸur.\n",
    "Aktif Eleman: Modelin katmanlarÄ± ve aktivasyon fonksiyonlarÄ±dÄ±r.\n",
    "Ã–rneÄŸin, model(inputs) Ã§aÄŸrÄ±ldÄ±ÄŸÄ±nda, bu iÅŸlem gerÃ§ekleÅŸir.\n",
    "\n",
    "Loss Function\n",
    "Ne olur?\n",
    "Modelin tahminleri (preds) ve gerÃ§ek hedefler (targets) arasÄ±ndaki fark hesaplanÄ±r.\n",
    "Bu fark, bir kayÄ±p fonksiyonu (loss function) tarafÄ±ndan bir Ã¶lÃ§Ã¼ye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.\n",
    "Ã–rneÄŸin, MSE, CrossEntropyLoss, BinaryCrossEntropy.\n",
    "Aktif Eleman: Loss function.\n",
    "    \n",
    "Backward Pass (Backpropagation)\n",
    "Ne olur?\n",
    "KayÄ±p fonksiyonu, model parametrelerine (aÄŸÄ±rlÄ±klar ve biaslar) gÃ¶re tÃ¼revlenir (gradyanlar hesaplanÄ±r).\n",
    "Bu iÅŸlem, otomatik tÃ¼revleme (autograd) mekanizmasÄ± tarafÄ±ndan gerÃ§ekleÅŸtirilir.\n",
    "Aktif Elemanlar: PyTorch gibi framework'lerin loss.backward() metodu.\n",
    "Bu aÅŸamada, gradyanlar w.grad ve b.grad gibi deÄŸiÅŸkenlerde saklanÄ±r.\n",
    "\n",
    "Optimizer\n",
    "Ne yapar?\n",
    "Backpropagation'Ä±n Ã¼rettiÄŸi gradyanlarÄ± kullanarak model parametrelerini gÃ¼nceller.\n",
    "Gradyanlar, optimizer'Ä±n algoritmasÄ±na (SGD, Adam, RMSprop vb.) baÄŸlÄ± olarak iÅŸlenir.\n",
    "FormÃ¼l:\n",
    "ğœƒ\n",
    "=\n",
    "ğœƒ\n",
    "âˆ’\n",
    "ğœ‚\n",
    "â‹…\n",
    "âˆ‡\n",
    "ğ½\n",
    "(\n",
    "ğœƒ\n",
    ")\n",
    "Î¸=Î¸âˆ’Î·â‹…âˆ‡J(Î¸)\n",
    "ğœƒ\n",
    "Î¸: Model parametreleri (Ã¶rneÄŸin aÄŸÄ±rlÄ±klar, biaslar).\n",
    "ğœ‚\n",
    "Î·: Ã–ÄŸrenme oranÄ±.\n",
    "âˆ‡\n",
    "ğ½\n",
    "(\n",
    "ğœƒ\n",
    ")\n",
    "âˆ‡J(Î¸): KayÄ±p fonksiyonunun gradyanÄ±.\n",
    "Aktif Elemanlar: opt.step().\n",
    "Optimizer'Ä±n RolÃ¼\n",
    "Backpropagation SonrasÄ± Gelen Ä°ÅŸlem:\n",
    "Backpropagation, sadece gradyanlarÄ± hesaplar. Modelin parametrelerini deÄŸiÅŸtirmez.\n",
    "Optimizasyon iÅŸlemi, bu gradyanlarÄ± kullanarak parametreleri gÃ¼nceller.\n",
    "KaybÄ± Minimize Etmek:\n",
    "Gradyanlar, kayÄ±p fonksiyonunun eÄŸimini temsil eder. Optimizer, bu eÄŸimi takip ederek kaybÄ± minimize eder.\n",
    "Model bu ÅŸekilde daha doÄŸru tahminler yapmayÄ± Ã¶ÄŸrenir.\n",
    "Forward, Loss, Backward ve Optimizer Ä°liÅŸkisi\n",
    "Bu iÅŸlemleri bir araya getirirsek:\n",
    "\n",
    "Forward Pass:\n",
    "Model tahmin yapar. preds = model(inputs)\n",
    "Loss Hesaplama:\n",
    "KayÄ±p hesaplanÄ±r. loss = loss_fn(preds, targets)\n",
    "Backward Pass:\n",
    "Gradyanlar hesaplanÄ±r. loss.backward()\n",
    "Optimizer:\n",
    "Parametreler gÃ¼ncellenir. opt.step()\n",
    "Gradyanlar sÄ±fÄ±rlanÄ±r. opt.zero_grad()\n",
    "SonuÃ§\n",
    "Optimizer, backpropagation'dan sonra gradyanlarÄ± kullanarak parametreleri gÃ¼ncelleyen bileÅŸendir.\n",
    "Ancak tahmin iÅŸlemini yapan forward pass, kaybÄ± hesaplayan loss function, ve gradyanlarÄ± hesaplayan backpropagation birbirin\n",
    "den baÄŸÄ±msÄ±z Ã§alÄ±ÅŸÄ±r ama birbiriyle iliÅŸkilidir. Optimizasyon iÅŸlemi, tÃ¼m bu adÄ±mlarÄ± bir araya getirerek modeli eÄŸitir.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
